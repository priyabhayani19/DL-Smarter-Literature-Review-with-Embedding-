{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyabhayani19/DL-Smarter-Literature-Review-with-Embedding-/blob/main/Deep_Learning_Powered_Literature_Review_Automating_Knowledge_Synthesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Collect Academic Papers Information to Excel"
      ],
      "metadata": {
        "id": "_FgYIrD5u_Hn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1Collect Academic Papers Information and Save to json File"
      ],
      "metadata": {
        "id": "zCa6Bd05vigF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQTYeQ3wZKZe"
      },
      "source": [
        "Install the Findpaper library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qie-sreZEUj"
      },
      "outputs": [],
      "source": [
        "!pip install findpapers\n",
        "!pip show findpapers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taf1xHRPZRQR"
      },
      "source": [
        "Check the Findpaper Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LWLR2fDZSBE"
      },
      "outputs": [],
      "source": [
        "!findpapers version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsksK7zRZXPw"
      },
      "source": [
        "Directory path on Google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJ3g5NLaZZpW"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "directory_path = '/content/drive/MyDrive/DL' #@param {type:\"string\"}\n",
        "if not os.path.exists(directory_path):\n",
        "    os.makedirs(directory_path)\n",
        "\n",
        "# Set API Keys in Environment Variables (here's your provided keys)\n",
        "os.environ[\"SCOPUS_API_KEY\"] = \"7d18ea0a338c85deda997ada06dfa686\"\n",
        "\n",
        "#  Retrieve them when needed (for your program use)\n",
        "SCOPUS_TOKEN = os.getenv(\"SCOPUS_API_KEY\")\n",
        "\n",
        "# Confirm they are set (optional — for testing)\n",
        "print(\"Scopus Token:\", SCOPUS_TOKEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJTNC_DzZvpD"
      },
      "source": [
        "Set the Parameters for Findpaper (If you want to search Scopus and IEEE, you need to enter the API key.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvWqBvCnZwP4"
      },
      "outputs": [],
      "source": [
        "#QUERY = \"[Virtual Reality] OR [Augmented Reality] OR ([Extended Reality] OR [Mixed Reality])\"  #@param {type:\"string\"}\n",
        "QUERY = '([Virtual Reality] OR [Augmented Reality] OR [Extended Reality] OR [Mixed Reality]) AND ([Therapy] OR [Treatment] OR [Intervention] OR [Rehabilitation] OR [Outcome]) AND ([Mental] OR [Psychological] OR [Emotional] OR [Addiction] OR [Alcohol] OR [Exposure] OR [Behavior] OR [Disorder]) AND NOT [Physical]'\n",
        "folder_name = \"search_rlhf_all\"  #@param {type:\"string\"}\n",
        "\n",
        "FOLDER = f'{directory_path}/{folder_name}.json'\n",
        "\n",
        "NUM = 1000  # @param {type:\"slider\", min:10, max:1000, step:1}\n",
        "\n",
        "SINCE = '2013-01-01'  #@param {type:\"date\"}\n",
        "UNTIL = '2025-05-01'  #@param {type:\"date\"}\n",
        "\n",
        "#@markdown Select Journals:\n",
        "\n",
        "pubmed = True  #@param {type:\"boolean\"}\n",
        "arxiv = True #@param {type:\"boolean\"}\n",
        "scopus = True #@param {type:\"boolean\"}\n",
        "\n",
        "selected_databases = []\n",
        "\n",
        "if pubmed:\n",
        "    selected_databases.append(\"pubmed\")\n",
        "if arxiv:\n",
        "    selected_databases.append(\"arxiv\")\n",
        "if scopus:\n",
        "    selected_databases.append(\"scopus\")\n",
        "\n",
        "DATABASES = \",\".join(selected_databases)\n",
        "\n",
        "selected_publication_types = [\"journal\"]\n",
        "\n",
        "PUBLICATION_TYPE = \",\".join(selected_publication_types)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuvxiiWAZ5ep"
      },
      "source": [
        "Execute the Findpaper command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Pdu8NoYbZ6Ka"
      },
      "outputs": [],
      "source": [
        "#Run findpapers CLI command with tokens for Scopus and IEEE\n",
        "!findpapers search {FOLDER} --query \"{QUERY}\" --limit-db {NUM} --since {SINCE} --until {UNTIL} --databases {DATABASES} --token-scopus {SCOPUS_TOKEN} --token-ieee {IEEE_TOKEN}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzE-Y4efrTJS"
      },
      "source": [
        "## 1.2 1.2 Export to an Excel from json file\n",
        "\n",
        "Loads paper metadata from a JSON file.\n",
        "- Extracts relevant fields: Title, Year, Abstract, Authors, Databases, Publisher, Journal, Keywords, DOI, Citations.\n",
        "- Removes duplicate papers based on title similarity (using a customizable threshold).\n",
        "-Saves the cleaned dataset into an Excel file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xjg-kdlSrNH8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "def similar(a, b):\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "def preprocess_title(title):\n",
        "    return title.lower().replace(\" \", \"\")\n",
        "\n",
        "def remove_duplicate_titles(df, threshold=0.9):\n",
        "    to_drop = []\n",
        "    for i in range(len(df)):\n",
        "        title_i = preprocess_title(df.iloc[i]['Title'])\n",
        "        for j in range(i+1, len(df)):\n",
        "            if j in to_drop:\n",
        "                continue\n",
        "            title_j = preprocess_title(df.iloc[j]['Title'])\n",
        "            sim = similar(title_i, title_j)\n",
        "            if sim >= threshold:\n",
        "                to_drop.append(j)\n",
        "    return df.drop(df.index[to_drop])\n",
        "\n",
        "# Specify the path to your JSON file\n",
        "folder_name = \"search_rlhf_all\"  #@param {type:\"string\"}\n",
        "file_path = f'{directory_path}/{folder_name}.json'\n",
        "\n",
        "# Load the JSON file\n",
        "with open(file_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Initialize a list for the DataFrame\n",
        "data_for_df = []\n",
        "\n",
        "# Extract information from each paper\n",
        "\n",
        "for paper in data.get(\"papers\", []):\n",
        "    # Extracting the required fields from each paper\n",
        "    title = paper.get(\"title\", \"\")\n",
        "    year = paper.get(\"publication_date\", \"\")\n",
        "    abstract = paper.get(\"abstract\", \"\")\n",
        "    authors = \"/ \".join(paper.get(\"authors\", []))  # Joining authors list into a single string\n",
        "    databases = \"/ \".join(paper.get(\"databases\", []))  # Joining databases list into a single string\n",
        "\n",
        "    # Accessing nested 'publisher' and 'title' fields within 'publication'\n",
        "    publication_info = paper.get(\"publication\", {}) or {}\n",
        "    publisher = publication_info.get(\"publisher\", \"\") if publication_info else \"\"\n",
        "    journal = publication_info.get(\"title\", \"\") if publication_info else \"\"\n",
        "    keywords = \"/ \".join(paper.get(\"keywords\", []))  # Joining keywords list into a single string\n",
        "    doi = paper.get(\"doi\", \"\")\n",
        "    citations = paper.get(\"citations\", \"\")\n",
        "\n",
        "    # Appending the extracted information as a dictionary to the list\n",
        "    data_for_df.append({\n",
        "        \"Title\": title,\n",
        "        \"Year\": year,\n",
        "        \"Abstract\": abstract,\n",
        "        \"Authors\": authors,\n",
        "        \"Databases\": databases,\n",
        "        \"Publisher\": publisher,\n",
        "        \"Journal\": journal,  # Including publication title\n",
        "        \"Keywords\": keywords,\n",
        "        \"DOI\": doi,\n",
        "        \"Citations\": citations\n",
        "    })\n",
        "\n",
        "# Create a DataFrame from the list of dictionaries\n",
        "df = pd.DataFrame(data_for_df)\n",
        "df = remove_duplicate_titles(df)\n",
        "\n",
        "# Specify the output filename and path\n",
        "output_filename = \"Output\"  #@param {type:\"string\"}\n",
        "output_file = f'{directory_path}/{output_filename}.xlsx'\n",
        "\n",
        "# Save the DataFrame as an Excel file\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(\"Excel file has been saved:\", output_file)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIy24lUaFGfm"
      },
      "source": [
        "#2. Impliment The Embendding Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INDPayG-FKlm"
      },
      "source": [
        "##2.1 Data Treatment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBnzU-7TFTWs"
      },
      "outputs": [],
      "source": [
        "pip install langdetect\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMe_BDqQFYll"
      },
      "source": [
        "### Cleans and normalizes text\n",
        "\n",
        "Cleans and normalizes text data by removing noise, non-English content, and formatting inconsistencies. Prepares abstracts for embedding generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LArs2CfRvq99"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    # Check if text is NaN or None and return empty string\n",
        "    if pd.isnull(text):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = BeautifulSoup(text, \"lxml\").get_text()\n",
        "\n",
        "    # Check if text is \"[No abstract available]\" and return empty string if true\n",
        "    if text == \"[No abstract available]\":\n",
        "        return \"\"\n",
        "\n",
        "    # Detect the language of the text and return empty string if not English\n",
        "    try:\n",
        "        if detect(text) != 'en':\n",
        "            return \"\"\n",
        "    except LangDetectException:\n",
        "        # In case language detection fails, return empty string\n",
        "        return \"\"\n",
        "\n",
        "    # Define patterns to remove (leading words and optional punctuation)\n",
        "    patterns_to_remove = [\n",
        "        r'^Objective[: \\.]*\\s*',\n",
        "        r'^Importance[:\\.]*\\s*',\n",
        "        r'^Background[:\\.]*\\s*'\n",
        "    ]\n",
        "\n",
        "    # Remove defined patterns\n",
        "    for pattern in patterns_to_remove:\n",
        "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove specific unwanted characters\n",
        "    stop_words = ['\\x0c', '\\n']\n",
        "    for stop_word in stop_words:\n",
        "        text = text.replace(stop_word, ' ')\n",
        "\n",
        "    # Keep only alphabetic characters and replace others with a space\n",
        "    clean_text = re.sub('[^a-zA-Z]+', ' ', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    return clean_text.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c967tg9Fvyb"
      },
      "source": [
        "###Import the necessary libraries\n",
        "\n",
        "- Data handling with pandas and numpy\n",
        "- Text cleaning and preprocessing (re, string, BeautifulSoup)\n",
        "- Language detection (langdetect)\n",
        "- TF-IDF vectorization (scikit-learn)\n",
        "- Clustering techniques: KMeans and MiniBatchKMeans\n",
        "- Dimensionality reduction: PCA and t-SNE\n",
        "- Data visualization: matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMGKpNIeF5RM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from bs4 import BeautifulSoup\n",
        "from langdetect import detect, LangDetectException\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data, Clean Abstracts, and Prepare DataFrame for Text Analysis"
      ],
      "metadata": {
        "id": "1rDSFrCx1P0o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0tmno9VF-Q1"
      },
      "outputs": [],
      "source": [
        "output_file = f'{directory_path}/{output_filename}.xlsx'\n",
        "df = pd.read_excel(output_file, usecols=['Title', 'Year', 'Abstract', 'Authors', 'Databases', 'Publisher', 'Journal', 'Keywords', 'DOI', 'Citations'])\n",
        "\n",
        "df['clean_abstract'] = df['Abstract'].apply(clean_text)\n",
        "\n",
        "# Find the index of the 'Abstract' column and insert 'clean_abstract' right after it\n",
        "abstract_col_index = df.columns.get_loc('Abstract') + 1\n",
        "df.insert(abstract_col_index, 'Clean Abstract', df.pop('clean_abstract'))\n",
        "\n",
        "df['ID'] = range(len(df))\n",
        "\n",
        "# Filter out rows where 'clean_abstract' is empty\n",
        "filtered_df = df[df['Clean Abstract'].str.strip().astype(bool)].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c8qyp4OGmOe"
      },
      "source": [
        "## 2.2 Install Sentence Transformers library:\n",
        "\n",
        "- This library allows you to generate sentence embeddings using models E5.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8NP69fTuGi_t"
      },
      "outputs": [],
      "source": [
        "pip install sentence-transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FYsvNmCG_Dl"
      },
      "source": [
        "### Generate Sentence Embeddings Using E5 Model\n",
        "\n",
        "Generate sentence embeddings for cleaned abstracts using the E5 model,\n",
        "store embeddings in the DataFrame, and export the results to a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nf_-WjkiwSC6"
      },
      "outputs": [],
      "source": [
        "# Import SentenceTransformer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Initialize E5 model\n",
        "model = SentenceTransformer('intfloat/e5-base')\n",
        "\n",
        "# Encode the non-empty 'Clean Abstracts' to sentence embeddings\n",
        "\n",
        "if not filtered_df.empty:\n",
        "    texts = filtered_df['Clean Abstract'].tolist()\n",
        "    embeddings = model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "    # Add embeddings to DataFrame as a list of lists\n",
        "    filtered_df['Embeddings'] = embeddings.tolist()\n",
        "\n",
        "    # Export DataFrame to CSV\n",
        "    filtered_df.to_csv('abstracts_with_embeddings.csv', index=False)\n",
        "\n",
        "    print(\"File exported successfully!\")\n",
        "        # Display the first 20 rows of the updated DataFrame\n",
        "else:\n",
        "    print(\"No data to process.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0JRl4kJwV06"
      },
      "source": [
        "Save Cleaned Data and display the top 20 data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5d8oVv-1g_Q",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Save the embeddings to a .xlsx file with the full path\n",
        "\n",
        "output_excel_path = f'{directory_path}/abstracts_with_embeddings.xlsx'\n",
        "\n",
        "filtered_df.to_excel(output_excel_path, index=False)\n",
        "\n",
        "print(f\"Filtered data with embeddings saved to: {output_excel_path}\")\n",
        "filtered_df.head(20)  # Display the first few rows for verification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFGQ49-uVLNi"
      },
      "source": [
        "## 2.3 Find the Cosine Similariy\n",
        "\n",
        "Conduct a semantic similarity search by comparing a query embedding against\n",
        "precomputed abstract embeddings using cosine similarity. Filter and rank\n",
        "results above a similarity threshold, export the top matches to Excel, and\n",
        "display a preview of the most relevant papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkO5NkMueOIn"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "\n",
        "# 1. Define your retrieval query\n",
        "query = (\n",
        "    \"([Virtual Reality] OR [Augmented Reality] OR [Extended Reality] OR [Mixed Reality]) \"\n",
        "    \"AND ([Therapy] OR [Treatment] OR [Intervention] OR [Rehabilitation] OR [Outcome]) \"\n",
        "    \"AND ([Mental] OR [Psychological] OR [Emotional] OR [Addiction] OR [Alcohol] OR [Exposure] OR [Behavior] OR [Disorder]) \"\n",
        "    \"AND NOT [Physical]\"\n",
        ")\n",
        "query_emb = model.encode(query)\n",
        "\n",
        "# 2. Ensure we have embeddings to compare\n",
        "embeddings_column = filtered_df.get('Embeddings')\n",
        "if embeddings_column is None or embeddings_column.dropna().empty:\n",
        "    print(\"No embeddings available in `filtered_df['Embeddings']` to compute similarity.\")\n",
        "else:\n",
        "    # Stack the embedding vectors\n",
        "    emb_list = embeddings_column.dropna().tolist()\n",
        "    emb_matrix = np.vstack(emb_list)\n",
        "\n",
        "    # 3. Compute cosine similarity\n",
        "    filtered_df['Similarity'] = cosine_similarity([query_emb], emb_matrix)[0]\n",
        "\n",
        "    # 4. Apply threshold filtering\n",
        "    threshold = 0.80\n",
        "    result_df = filtered_df[filtered_df['Similarity'] >= threshold].copy()\n",
        "\n",
        "    if result_df.empty:\n",
        "        print(f\"No papers found with similarity ≥ {threshold}.\")\n",
        "    else:\n",
        "        # 5. Sort, cap, and save\n",
        "        top_n = 1000\n",
        "        result_df = result_df.sort_values(by='Similarity', ascending=False).head(top_n)\n",
        "\n",
        "        drive_folder = '/content/drive/MyDrive/DL/'\n",
        "        output_path = drive_folder + 'most_similar_paperse5.xlsx'\n",
        "        result_df.to_excel(output_path, index=False)\n",
        "        print(f\"Saved {len(result_df)} papers with similarity ≥ {threshold} to:\\n  {output_path}\")\n",
        "\n",
        "      # 6 Additionally: display top 20 papers from entire filtered_df by similarity\n",
        "    filtered_df_sorted = filtered_df.sort_values(by='Similarity', ascending=False).reset_index(drop=True)\n",
        "    print(filtered_df_sorted[['Title', 'Year', 'Databases','Similarity']].head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keyword-Based Relevance Filtering of Papers"
      ],
      "metadata": {
        "id": "b-Qx-fRL7Djt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPESaxZwrnNZ"
      },
      "outputs": [],
      "source": [
        "query_keywords = [\n",
        "    \"virtual reality\", \"augmented reality\", \"extended reality\", \"mixed reality\",\n",
        "    \"therapy\", \"treatment\", \"intervention\", \"rehabilitation\", \"outcome\",\n",
        "    \"mental\", \"psychological\", \"emotional\", \"addiction\", \"alcohol\", \"exposure\",\n",
        "    \"behavior\", \"disorder\"\n",
        "]\n",
        "\n",
        "# Make sure Keywords column is string type and lowercase\n",
        "filtered_df['Keywords'] = filtered_df['Keywords'].fillna(\"\").str.lower()\n",
        "\n",
        "# Create a mask (boolean Series) that is True if any keyword appears in the 'Keywords' column\n",
        "def contains_any_keyword(text, keywords):\n",
        "    return any(keyword in text for keyword in keywords)\n",
        "\n",
        "filtered_df['IsRelevant'] = filtered_df['Keywords'].apply(lambda x: contains_any_keyword(x, query_keywords))\n",
        "\n",
        "# Example: Show top 10 sorted by similarity, with relevance info\n",
        "top10 = filtered_df.sort_values(by='Similarity', ascending=False).head(10).copy()\n",
        "top10['IsRelevant'] = top10['Keywords'].apply(lambda x: contains_any_keyword(x, query_keywords))\n",
        "\n",
        "print(top10[['Title', 'Similarity', 'IsRelevant']])\n",
        "\n",
        "#print(filtered_df['IsRelevant'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Clustring (K-Means)"
      ],
      "metadata": {
        "id": "YSKxPvFA7cyf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCS2ae9lOTEN"
      },
      "source": [
        "## 3.1 Load Dataset and Parse Embeddings for Clustering and Visualization\n",
        "\n",
        "Parse embeddings stored as strings in the Excel file, convert them to numeric arrays,\n",
        "and prepare the data for clustering and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iFuVYgsOMuC"
      },
      "outputs": [],
      "source": [
        "#  Load dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "import torch\n",
        "\n",
        "# Load df (for reference)\n",
        "df = pd.read_excel('/content/drive/MyDrive/DL/most_similar_paperse5.xlsx', sheet_name='Sheet1')\n",
        "\n",
        "# Define function to parse embeddings\n",
        "def parse_embeddings(embeddings):\n",
        "    parsed = []\n",
        "    for emb in embeddings:\n",
        "        if isinstance(emb, str):\n",
        "            try:\n",
        "                emb_list = ast.literal_eval(emb)\n",
        "                if not isinstance(emb_list, (list, tuple)):\n",
        "                    raise ValueError(f\"Parsed embedding is not a list: {emb_list}\")\n",
        "                parsed.append(emb_list)\n",
        "            except (ValueError, SyntaxError) as e:\n",
        "                raise ValueError(f\"Failed to parse embedding: {emb}, error: {e}\")\n",
        "        elif isinstance(emb, (list, tuple)):\n",
        "            parsed.append(emb)\n",
        "        else:\n",
        "            raise ValueError(f\"Embedding is neither a string nor a list: {emb}\")\n",
        "    parsed_array = np.array(parsed, dtype=float)\n",
        "    if len(set(len(row) for row in parsed)) != 1:\n",
        "        raise ValueError(\"Embeddings have inconsistent dimensions\")\n",
        "    return parsed_array\n",
        "\n",
        "# Assuming filtered_df is already created or filtered from df\n",
        "text = parse_embeddings(df['Embeddings'])\n",
        "print(f\"Embeddings shape: {text.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MoEV1yEOehx"
      },
      "source": [
        "### 3.2 Elbow Method\n",
        "\n",
        "Estimating the optimal number of clusters using the Sum of Squared Errors (SSE) involves analyzing the clustering performance as the number of clusters changes.\n",
        "\n",
        "**SSE (Sum of Squared Errors):** SSE is a common metric used to evaluate the performance of clustering algorithms. It is calculated as the sum of the squared distances between each data point and the centroid of the cluster it belongs to.**bold text**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW8tm6cROqNe"
      },
      "outputs": [],
      "source": [
        "# Function to find optimal clusters via Elbow Method\n",
        "def find_optimal_clusters(data, max_k):\n",
        "    iters = range(2, max_k + 1, 2)\n",
        "    sse = []\n",
        "    for k in iters:\n",
        "        sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(data).inertia_)\n",
        "        print(f'Fit {k} clusters')\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(iters, sse, marker='o')\n",
        "    plt.xlabel('Cluster Centers')\n",
        "    plt.xticks(iters)\n",
        "    plt.ylabel('SSE')\n",
        "    plt.title('Elbow Plot for Optimal Clusters')\n",
        "    plt.show()\n",
        "\n",
        "# Find elbow\n",
        "cluster_num = 30\n",
        "find_optimal_clusters(text, cluster_num)\n",
        "\n",
        "# Perform final clustering\n",
        "n_clusters = 10  # Adjust based on elbow plot\n",
        "clusters = MiniBatchKMeans(n_clusters=n_clusters, init_size=1024, batch_size=2048, random_state=20).fit_predict(text)\n",
        "\n",
        "# Add cluster labels to dataframe\n",
        "df['Cluster'] = clusters\n",
        "\n",
        "# Save to Excel\n",
        "cluster_filename = \"Cluster\"\n",
        "directory_path = '/content/drive/MyDrive/DL/'\n",
        "cluster_file = f'{directory_path}/{cluster_filename}.xlsx'\n",
        "df.drop(columns=['ID']).to_excel(cluster_file, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8utdwiuDOvJX"
      },
      "source": [
        "### 3.3 2D t-SNE Visualization\n",
        "\n",
        "Visualize high-dimensional embeddings in 2D using PCA for dimensionality reduction followed by t-SNE, with points colored by cluster labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6p0fhPkOwC7"
      },
      "outputs": [],
      "source": [
        "# 2D TSNE Visualization\n",
        "def plot_tsne_2d(data, labels):\n",
        "    if isinstance(data, torch.Tensor):\n",
        "        data_np = data.numpy()\n",
        "    else:\n",
        "        data_np = data\n",
        "\n",
        "    if data_np.shape[1] < 2:\n",
        "        raise ValueError(\"TSNE requires at least 2 features\")\n",
        "\n",
        "    sample_size = min(3000, data_np.shape[0])\n",
        "    max_items = np.random.choice(range(data_np.shape[0]), size=sample_size, replace=False)\n",
        "    n_components_pca = min(20, data_np.shape[1])\n",
        "\n",
        "    # Apply PCA before TSNE for efficiency\n",
        "    pca_result = PCA(n_components=n_components_pca).fit_transform(data_np[max_items, :])\n",
        "\n",
        "    # Apply TSNE\n",
        "    tsne = TSNE(perplexity=15, n_components=2, init='pca', max_iter=1000, learning_rate=100, random_state=23)\n",
        "    tsne_result = tsne.fit_transform(pca_result)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(11, 8))\n",
        "    scatter = plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=labels[max_items], cmap='tab10', s=50, alpha=0.6)\n",
        "    for i, idx in enumerate(max_items):\n",
        "        plt.text(tsne_result[i, 0], tsne_result[i, 1], str(labels[idx]), fontsize=10, alpha=0.7)\n",
        "    plt.colorbar(scatter, label='Cluster')\n",
        "    plt.xlabel('TSNE Component 1')\n",
        "    plt.ylabel('TSNE Component 2')\n",
        "    plt.title('2D TSNE Visualization of Clusters')\n",
        "    plt.show()\n",
        "\n",
        "# Visualize the clustering\n",
        "plot_tsne_2d(text, clusters)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmuU9gujhcky"
      },
      "source": [
        "This bar Graph showing the number of control papers in each cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGjnZYh2gzp8"
      },
      "outputs": [],
      "source": [
        "#Display the number of papers classified into each cluster as a bar graph\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "df = pd.read_excel(cluster_file, usecols=['Cluster'])\n",
        "cluster_list = pd.to_numeric(df['Cluster'], errors='coerce').dropna().astype(int).tolist()\n",
        "\n",
        "cluster_counts = pd.Series(cluster_list).value_counts()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "cluster_counts.plot(kind='bar')\n",
        "plt.xlabel('Number of papers in each cluster')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "top30_clusters_df = cluster_counts.head(30).to_frame('Frequency')\n",
        "display(HTML(top30_clusters_df.to_html()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Evoluation"
      ],
      "metadata": {
        "id": "BLk7zdpDAEMN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O28O_hXWhfz9"
      },
      "source": [
        "## 4.1 Keyword extraction From Each Cluster\n",
        "\n",
        "Extracts and cleans keywords for each cluster, aggregates their frequencies,and returns the top N keywords per cluster. This helps summarize the main topics represented in each cluster for better interpretation of clustering results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def get_top_keywords_per_cluster(df, top_n=10):\n",
        "    cluster_keywords = {}\n",
        "\n",
        "    for cluster in sorted(df['Cluster'].unique()):\n",
        "        keywords_series = df[df['Cluster'] == cluster]['Keywords'].dropna()\n",
        "\n",
        "        all_keywords = []\n",
        "        for keywords in keywords_series:\n",
        "            if isinstance(keywords, str):\n",
        "                # Standardize separators\n",
        "                clean_string = re.sub(r'(\\/ ?n ?|/n|/|\\n|\\t|\\r)', ',', keywords)\n",
        "                # Split by comma\n",
        "                kws = [kw.strip() for kw in clean_string.split(',')]\n",
        "                # Clean each keyword: remove leading 'N ' or 'n ' and remove standalone 'n' or 'N'\n",
        "                cleaned_kws = []\n",
        "                for kw in kws:\n",
        "                    # Remove leading 'N ' or 'n ' if exists\n",
        "                    kw_cleaned = re.sub(r'^[Nn]\\s+', '', kw)\n",
        "                    # Remove standalone 'n' or 'N' entries\n",
        "                    if kw_cleaned and kw_cleaned.lower() != 'n':\n",
        "                        cleaned_kws.append(kw_cleaned)\n",
        "                all_keywords.extend(cleaned_kws)\n",
        "            elif not pd.isna(keywords):\n",
        "                print(f\"Warning: Found non-string keyword in cluster {cluster}: {keywords}\")\n",
        "\n",
        "        # Merge duplicates ignoring case and sum counts\n",
        "        merged_counts = defaultdict(int)\n",
        "        original_casing = {}\n",
        "\n",
        "        for kw in all_keywords:\n",
        "            kw_clean = kw.strip()\n",
        "            kw_lower = kw_clean.lower()\n",
        "            merged_counts[kw_lower] += 1\n",
        "            # Keep the first casing seen\n",
        "            if kw_lower not in original_casing:\n",
        "                original_casing[kw_lower] = kw_clean\n",
        "\n",
        "        # Convert to list of (original_case_keyword, count)\n",
        "        merged_list = [(original_casing[kw_lower], count) for kw_lower, count in merged_counts.items()]\n",
        "\n",
        "        # Sort by count descending and pick top_n\n",
        "        merged_list.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_keywords = merged_list[:top_n]\n",
        "\n",
        "        cluster_keywords[cluster] = top_keywords\n",
        "\n",
        "    return cluster_keywords\n",
        "\n",
        "# Load your DataFrame\n",
        "df = pd.read_excel(cluster_file, usecols=['Cluster', 'Keywords'])\n",
        "\n",
        "# Get top keywords by cluster\n",
        "top_keywords_by_cluster = get_top_keywords_per_cluster(df)\n",
        "\n",
        "# Print result\n",
        "for cluster, keywords in top_keywords_by_cluster.items():\n",
        "    print(f\"\\nCluster {cluster}:\")\n",
        "    if keywords:\n",
        "        print(\", \".join([f\"{kw} ({count})\" for kw, count in keywords]))\n",
        "    else:\n",
        "        print(\"No keywords available in this cluster.\")\n"
      ],
      "metadata": {
        "id": "PzbFXKKYRQVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ploting Top 15 keyword from each cluster in Pai chart\n",
        "\n",
        "Generates a pie chart visualizing the top 15 most frequent keywords across all clusters."
      ],
      "metadata": {
        "id": "3SALCbMnBn0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def plot_top_15_unique_keywords_pie(top_keywords_by_cluster):\n",
        "    combined_counter = Counter()\n",
        "\n",
        "    # Combine keywords from all clusters, merging duplicates ignoring case\n",
        "    for cluster_keywords in top_keywords_by_cluster.values():\n",
        "        for keyword, count in cluster_keywords:\n",
        "            normalized_kw = keyword.lower()\n",
        "            combined_counter[normalized_kw] += count\n",
        "\n",
        "    # Get top 15 keywords by frequency\n",
        "    top_15 = combined_counter.most_common(15)\n",
        "\n",
        "    # For better labels, capitalize keywords nicely\n",
        "    labels = [kw.title() for kw, _ in top_15]\n",
        "    sizes = [count for _, count in top_15]\n",
        "\n",
        "        # Explode small slices a bit to separate them visually (optional)\n",
        "    explode = [0.08 if size < max(sizes)*0.10 else 0 for size in sizes]\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    wedges, texts, autotexts = plt.pie(\n",
        "        sizes, labels=labels, autopct='%1.1f%%', startangle=190, pctdistance=0.8,\n",
        "        labeldistance=1.1\n",
        "    )\n",
        "\n",
        "    plt.setp(autotexts, size=8, weight=\"bold\")\n",
        "    plt.setp(texts, size=8)\n",
        "    plt.title('Keyword', fontsize=10)\n",
        "    plt.axis('equal')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Load your data\n",
        "df = pd.read_excel(cluster_file, usecols=['Cluster', 'Keywords'])\n",
        "\n",
        "# Assuming get_top_keywords_per_cluster is already defined and returns the dictionary\n",
        "top_keywords_by_cluster = get_top_keywords_per_cluster(df)\n",
        "\n",
        "# Plot pie chart with top 15 unique keywords merged case insensitively\n",
        "plot_top_15_unique_keywords_pie(top_keywords_by_cluster)\n"
      ],
      "metadata": {
        "id": "l65wTh6zS9rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsyD0RafhqL-"
      },
      "source": [
        "### 4.2 Cluster Summarization Using T5 (or BART)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the Libreary"
      ],
      "metadata": {
        "id": "FSnE4b5wC2Df"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHVhHei2hr0r",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "pip install transformers sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the Librery"
      ],
      "metadata": {
        "id": "7vMz3DNmDIwx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJicB9LQhx1b"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cluster-wise Abstract Summarization with T5 Model\n",
        "\n",
        "- Generates automatic summaries of abstracts for each cluster using a pre-trained T5 summarization model.\n",
        "- Groups abstracts by cluster, concatenates them, and produces a concise summary for each group.\n",
        "- The summaries are added to the DataFrame and saved to an updated Excel file.\n",
        "\n"
      ],
      "metadata": {
        "id": "hF3-hPp5DMA_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpZVaSuqRYYV",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Load the clustered DataFrame\n",
        "df = pd.read_excel('/content/drive/MyDrive/DL/Cluster.xlsx')\n",
        "df['Clean Abstract'] = df['Clean Abstract'].fillna(\"\")\n",
        "\n",
        "# Initialize summarizer (you can use 'facebook/bart-large-cnn' as an alternative)\n",
        "summarizer = pipeline(\"summarization\", model=\"t5-small\", tokenizer=\"t5-small\", framework=\"pt\")\n",
        "\n",
        "# Group abstracts by cluster\n",
        "cluster_texts = defaultdict(list)\n",
        "for _, row in df.iterrows():\n",
        "    cluster_texts[row['Cluster']].append(row['Clean Abstract'])\n",
        "\n",
        "# Combine all abstracts per cluster into one big string\n",
        "cluster_summaries = {}\n",
        "for cluster_id, abstracts in cluster_texts.items():\n",
        "    full_text = \" \".join(abstracts)\n",
        "\n",
        "    # Limit text size (transformers max input ~512-1024 tokens)\n",
        "    full_text = full_text[:3000]  # Adjust if needed\n",
        "\n",
        "    # Add prompt for T5\n",
        "    input_text = \"summarize: \" + full_text\n",
        "\n",
        "    try:\n",
        "        summary = summarizer(input_text, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n",
        "    except Exception as e:\n",
        "        summary = f\"Failed to summarize cluster {cluster_id}: {str(e)}\"\n",
        "\n",
        "    cluster_summaries[cluster_id] = summary\n",
        "\n",
        "# Print summaries\n",
        "print(\" Cluster Summaries:\")\n",
        "for cluster_id in sorted(cluster_summaries):\n",
        "    print(f\"\\nCluster {cluster_id} Summary:\\n{cluster_summaries[cluster_id]}\")\n",
        "\n",
        "# Optional: Add summary column back to DataFrame\n",
        "df['Cluster Summary'] = df['Cluster'].map(cluster_summaries)\n",
        "\n",
        "# Save to file\n",
        "output_path = '/content/drive/MyDrive/DL/Cluster.xlsx'\n",
        "df.to_excel(output_path, index=False)\n",
        "print(f\"\\n Saved file with cluster summaries to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Rank Papers by Similarity & Citations per Cluster\n",
        "\n",
        "Normalize Similarity and Citation Metrics, Compute Combined Scores, and Rank Papers per Cluster\n",
        "\n"
      ],
      "metadata": {
        "id": "Ca8u-3h22JPB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La6OYor_uGhm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load clustered DataFrame (with 'Cluster', 'Citations', and 'Similarity' columns)\n",
        "df = pd.read_excel('/content/drive/MyDrive/DL/Cluster.xlsx')\n",
        "\n",
        "# Handle missing values\n",
        "df['Citations'] = pd.to_numeric(df['Citations'], errors='coerce').fillna(0)\n",
        "df['Similarity'] = pd.to_numeric(df['Similarity'], errors='coerce').fillna(0)\n",
        "\n",
        "# Normalize both metrics for fair ranking (scale between 0 and 1)\n",
        "df['Citations_Norm'] = df['Citations'] / df['Citations'].max()\n",
        "df['Similarity_Norm'] = df['Similarity'] / df['Similarity'].max()\n",
        "\n",
        "# Combine both metrics: weighted average (adjust weights as needed)\n",
        "df['CombinedScore'] = (0.6 * df['Similarity_Norm']) + (0.4 * df['Citations_Norm'])\n",
        "\n",
        "# Rank within each cluster by CombinedScore (higher = better)\n",
        "df['Cluster Rank'] = df.groupby('Cluster')['CombinedScore']\\\n",
        "                       .rank(method='dense', ascending=False).astype(int)\n",
        "\n",
        "# Sort the whole DataFrame by cluster and rank\n",
        "df_sorted = df.sort_values(by=['Cluster', 'Cluster Rank'])\n",
        "\n",
        "# Display sample\n",
        "print(df_sorted[['Cluster', 'Cluster Rank', 'Title', 'Citations', 'Similarity']].head(20))\n",
        "\n",
        "# Save to Excel\n",
        "output_path = '/content/drive/MyDrive/DL/Cluster.xlsx'\n",
        "df_sorted.to_excel(output_path, index=False)\n",
        "print(f\"\\n Ranked results saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Publication Count by Year"
      ],
      "metadata": {
        "id": "XlkLmKsQyRNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(cluster_file, usecols=['Year'])\n",
        "\n",
        "df['Year'] = pd.to_datetime(df['Year'], format='%Y-%m-%d').dt.year\n",
        "\n",
        "year_counts = df['Year'].value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "year_counts.plot(kind='bar')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', linewidth=0.7)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "pd.set_option('display.max_colwidth', 50)\n",
        "year_df = year_counts.head(11).to_frame('Count')\n",
        "\n",
        "display(HTML(year_df.to_html(escape=False)))"
      ],
      "metadata": {
        "id": "caNsU8cYxAL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 Number of papers fetched from each database.\n",
        "\n"
      ],
      "metadata": {
        "id": "yPk5osqsyawk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(cluster_file, usecols=['Databases'])\n",
        "database_list = df['Databases'].dropna().apply(lambda x: x.split('/')).tolist()\n",
        "\n",
        "flat_database_list = [item.strip() for sublist in database_list for item in sublist]\n",
        "\n",
        "database_counts = pd.Series(flat_database_list).value_counts()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "database_counts.plot(kind='bar')\n",
        "plt.xlabel('Database')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "database_counts.head()\n",
        "\n",
        "pd.set_option('display.max_colwidth', 50)\n",
        "database_df = database_counts.head().to_frame('Count')\n",
        "\n",
        "display(HTML(database_df.to_html(escape=False)))"
      ],
      "metadata": {
        "id": "xr8GUJKSxGAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.6 Distribution of Papers Across Journals\n",
        "\n"
      ],
      "metadata": {
        "id": "CCKv4v8fykCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(cluster_file, usecols=['Journal'])\n",
        "journal_list = df['Journal'].dropna().apply(lambda x: x.split('/')).tolist()\n",
        "\n",
        "flat_journal_list = [item.strip() for sublist in journal_list for item in sublist]\n",
        "\n",
        "journal_counts = pd.Series(flat_journal_list).value_counts()\n",
        "top10_journals = journal_counts.head(10)\n",
        "\n",
        "def abbreviate_journal_name(name, max_length=20):\n",
        "    if len(name) > max_length:\n",
        "        return name[:max_length] + \"...\"\n",
        "    else:\n",
        "        return name\n",
        "\n",
        "abbreviated_journals = top10_journals.index.map(lambda x: abbreviate_journal_name(x))\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "top10_journals.plot(kind='bar')\n",
        "plt.xlabel('Journal')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(range(len(abbreviated_journals)), abbreviated_journals, rotation=90)\n",
        "plt.show()\n",
        "\n",
        "pd.set_option('display.max_colwidth', 50)\n",
        "top30_journals_df = journal_counts.head(30).to_frame('Count')\n",
        "\n",
        "display(HTML(top30_journals_df.to_html(escape=False)))"
      ],
      "metadata": {
        "id": "YuzP0m99xLD2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}